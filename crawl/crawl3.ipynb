{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요청(requests) => 결과가 html로 넘어오는 경우\n",
    "# html에서 원하는 정보 찾기 => 파싱\n",
    "\n",
    "# BeautifylSoup 라이브러리 사용: pip install beautifulsoup4\n",
    "# 파서\n",
    "# 기본 파서 html.parser 설치 필요 없음, lxml는 설치 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3 class=\"tit_view\" data-translation=\"true\">원희룡 \"양평군민들 죄송, 기다려달라…도박? 이재명 가족 ·측근이 좋아해\"</h3>\n",
      "원희룡 \"양평군민들 죄송, 기다려달라…도박? 이재명 가족 ·측근이 좋아해\"\n",
      "원희룡 \"양평군민들 죄송, 기다려달라…도박? 이재명 가족 ·측근이 좋아해\"\n",
      "{'class': ['tit_view'], 'data-translation': 'true'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"https://v.daum.net/v/20230707084356035\")\n",
    "# 두번째 인자로 '파서' 지정\n",
    "# html.parser, lxml, lxml-xml, html5lib 중 하나 사용\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "# 원하는 태그 뽑아내기\n",
    "# print(soup)\n",
    "# print(soup.head)\n",
    "# print(soup.title)\n",
    "# print(soup.body)\n",
    "print(soup.h3) # 처음 만나는 h3 태그 가져오기\n",
    "print(soup.h3.string) # h3 태그 안에 있는 순수 내용만 가져온다.\n",
    "print(soup.h3.get_text())\n",
    "print(soup.h3.attrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   H1 Area\n",
      "  </h1>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" data-io=\"test\" href=\"http://example.com/tillie\" id=\"link2\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ; and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# story.html 가져오기\n",
    "with open(\"./story.html\",\"r\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>The Dormouse's story</title>\n",
      "The Dormouse's story\n",
      "<head>\n",
      "<title>The Dormouse's story</title>\n",
      "</head>\n"
     ]
    }
   ],
   "source": [
    "# 태그를 이용한 정보 찾기\n",
    "\n",
    "# title 태그 찾기\n",
    "print(soup.title)\n",
    "# title 태그가 가지고 있는 내용 찾기\n",
    "print(soup.title.string)\n",
    "# title 태그의 부모 찾기: 타이틀 태그를 감싸고 있는 헤드 태그가 출력된다.\n",
    "print(soup.title.parent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>H1 Area</h1>\n",
      "H1 Area\n"
     ]
    }
   ],
   "source": [
    "# h1 태그 찾기\n",
    "print(soup.h1)\n",
    "print(soup.h1.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"title\">\n",
      "<b> The Dormouse's story </b>\n",
      "</p>\n",
      "None\n",
      "The Dormouse's story\n",
      "['title']\n"
     ]
    }
   ],
   "source": [
    "# p 태그 찾기: 가장 처음 만나는 p 태그 찾기\n",
    "print(soup.p)\n",
    "print(soup.string)\n",
    "print(soup.p.get_text().strip())\n",
    "print(soup.p[\"class\"]) # 하나만 있어도 결과가 리스트로 돌아온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b> The Dormouse's story </b>\n",
      " The Dormouse's story \n"
     ]
    }
   ],
   "source": [
    "# b 태그 찾기\n",
    "print(soup.b)\n",
    "print(soup.b.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>H1 Area</h1>\n",
      "H1 Area\n"
     ]
    }
   ],
   "source": [
    "# find(): 가장 처음 만나는 태그\n",
    "# find_all(): 모든 태그\n",
    "print(soup.find(\"h1\"))\n",
    "print(soup.find(\"h1\").string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"title\">\n",
      "<b> The Dormouse's story </b>\n",
      "</p>\n",
      "<p class=\"story\">\n",
      "      Once upon a time there were three little sisters; and their names were\n",
      "      <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "      ,\n",
      "      <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>\n",
      "      and\n",
      "      <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link2\"> Tillie </a>\n",
      "      ; and they lived at the bottom of a well.\n",
      "    </p>\n",
      "<p class=\"story\">...</p>\n"
     ]
    }
   ],
   "source": [
    "# 찾은 태그가 하나여도 결과는 무조건 리스트로 출력\n",
    "# print(soup.find_all(\"p\"))\n",
    "for tag in soup.find_all(\"p\"):\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"story\">\n",
      "      Once upon a time there were three little sisters; and their names were\n",
      "      <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "      ,\n",
      "      <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>\n",
      "      and\n",
      "      <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link2\"> Tillie </a>\n",
      "      ; and they lived at the bottom of a well.\n",
      "    </p>, <p class=\"story\">...</p>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all(\"p\", class_=\"story\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link2\"> Tillie </a>]\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link2\"> Tillie </a>]\n",
      "http://example.com/elsie\n",
      "http://example.com/lacie\n",
      "http://example.com/tillie\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all(\"a\"))\n",
    "print(soup.find_all(\"a\", class_=\"sister\"))\n",
    "\n",
    "for link in soup.find_all(\"a\"):\n",
    "    print(link.get(\"href\")) # 링크 안의 href만 가져오고 싶어요.\n",
    "\n",
    "print(soup.find_all(\"a\", limit=2)) # 그 중에서 앞의 두 개만 가져와라\n",
    "print(soup.find_all(\"a\", string=[\"Elsie\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n"
     ]
    }
   ],
   "source": [
    "# find(\"a\", class_=\"\", limit=2, string=[\"Elsie\"], id=\"\"), find_all():\n",
    "# class_: 클래스명 추가 지정\n",
    "# limit: 가져오는 개수 제한\n",
    "# string: 문자\n",
    "# id: id 속성\n",
    "\n",
    "a_tag = soup.find(\"a\", id=\"link1\", class_=\"sister\")\n",
    "print(a_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"sister\" data-io=\"test\" href=\"http://example.com/tillie\" id=\"link2\"> Tillie </a>\n"
     ]
    }
   ],
   "source": [
    "a_tag = soup.find(\"a\", attrs={\"data-io\":\"test\", \"class\":\"sister\"})\n",
    "print(a_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n",
      "Well, Prince, so Genoa and Lucca are now just family estates of the\n",
      "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
      "if you still try to defend the infamies and horrors perpetrated by\n",
      "that Antichrist- I really believe he is Antichrist- I will have\n",
      "nothing more to do with you and you are no longer my friend, no longer\n",
      "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
      "I have frightened you- sit down and tell me all the news.\n",
      "If you have nothing better to do, Count [or Prince], and if the\n",
      "prospect of spending an evening with a poor invalid is not too\n",
      "terrible, I shall be very charmed to see you tonight between 7 and 10-\n",
      "Annette Scherer.\n",
      "Heavens! what a virulent attack!\n",
      "First of all, dear friend, tell me how you are. Set your friend's\n",
      "mind at rest,\n",
      "Can one be well while suffering morally? Can one be calm in times\n",
      "like these if one has any feeling?\n",
      "You are\n",
      "staying the whole evening, I hope?\n",
      "And the fete at the English ambassador's? Today is Wednesday. I\n",
      "must put in an appearance there,\n",
      "My daughter is\n",
      "coming for me to take me there.\n",
      "I thought today's fete had been canceled. I confess all these\n",
      "festivities and fireworks are becoming wearisome.\n",
      "If they had known that you wished it, the entertainment would\n",
      "have been put off,\n",
      "Don't tease! Well, and what has been decided about Novosiltsev's\n",
      "dispatch? You know everything.\n",
      "What can one say about it?\n",
      "What has been decided? They have decided that\n",
      "Buonaparte has burnt his boats, and I believe that we are ready to\n",
      "burn ours.\n",
      "Oh, don't speak to me of Austria. Perhaps I don't understand\n",
      "things, but Austria never has wished, and does not wish, for war.\n",
      "She is betraying us! Russia alone must save Europe. Our gracious\n",
      "sovereign recognizes his high vocation and will be true to it. That is\n",
      "the one thing I have faith in! Our good and wonderful sovereign has to\n",
      "perform the noblest role on earth, and he is so virtuous and noble\n",
      "that God will not forsake him. He will fulfill his vocation and\n",
      "crush the hydra of revolution, which has become more terrible than\n",
      "ever in the person of this murderer and villain! We alone must\n",
      "avenge the blood of the just one.... Whom, I ask you, can we rely\n",
      "on?... England with her commercial spirit will not and cannot\n",
      "understand the Emperor Alexander's loftiness of soul. She has\n",
      "refused to evacuate Malta. She wanted to find, and still seeks, some\n",
      "secret motive in our actions. What answer did Novosiltsev get? None.\n",
      "The English have not understood and cannot understand the\n",
      "self-abnegation of our Emperor who wants nothing for himself, but only\n",
      "desires the good of mankind. And what have they promised? Nothing! And\n",
      "what little they have promised they will not perform! Prussia has\n",
      "always declared that Buonaparte is invincible, and that all Europe\n",
      "is powerless before him.... And I don't believe a word that Hardenburg\n",
      "says, or Haugwitz either. This famous Prussian neutrality is just a\n",
      "trap. I have faith only in God and the lofty destiny of our adored\n",
      "monarch. He will save Europe!\n",
      "I think,\n",
      "None\n",
      "In a moment. A propos,\n",
      "None\n",
      "I shall be delighted to meet them,\n",
      "But tell me,\n",
      "is it true that the Dowager Empress wants Baron Funke\n",
      "to be appointed first secretary at Vienna? The baron by all accounts\n",
      "is a poor creature.\n",
      "Baron Funke has been recommended to the Dowager Empress by her\n",
      "sister,\n",
      "Now about your family. Do you know that since your daughter came\n",
      "out everyone has been enraptured by her? They say she is amazingly\n",
      "beautiful.\n",
      "I often think,\n",
      "None\n",
      "Two such charming children. And really you appreciate\n",
      "them less than anyone, and so you don't deserve to have them.\n",
      "I can't help it,\n",
      "Lavater would have said I\n",
      "lack the bump of paternity.\n",
      "Don't joke; I mean to have a serious talk with you. Do you know I\n",
      "am dissatisfied with your younger son? Between ourselves\n",
      "he was mentioned at Her\n",
      "Majesty's and you were pitied....\n",
      "What would you have me do?\n",
      "You know I did all\n",
      "a father could for their education, and they have both turned out\n",
      "fools. Hippolyte is at least a quiet fool, but Anatole is an active\n",
      "one. That is the only difference between them.\n",
      "And why are children born to such men as you? If you were not a\n",
      "father there would be nothing I could reproach you with,\n",
      "I am your faithful slave and to you alone I can confess that my\n",
      "children are the bane of my life. It is the cross I have to bear. That\n",
      "is how I explain it to myself. It can't be helped!\n"
     ]
    }
   ],
   "source": [
    "# https://pythonscraping.com/pages/warandpeace.html 문서 읽어서 가져오기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://pythonscraping.com/pages/warandpeace.html\")\n",
    "# print(res.text)\n",
    "soup = BeautifulSoup(res.text, \"html.parser\") # lxml\n",
    "\n",
    "# print(soup.get_text()) # get_text(): html 파일에서 문자만 싹 갖고온다.\n",
    "\n",
    "# 등장인물 가져오기\n",
    "persons = soup.find_all(\"span\", class_=\"green\")\n",
    "for person in persons:\n",
    "    print(person.string)\n",
    "\n",
    "# 대사 가져오기\n",
    "scripts = soup.find_all(\"span\", class_=\"red\")\n",
    "for script in scripts:\n",
    "    print(script.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "황선우, 박태환 이후 12년만의 세계수영선수권 '금빛 역영' 도전\n",
      "이대호\n",
      "2023. 7. 10. 07:01\n",
      "(서울=연합뉴스) 이대호 기자 = 전 세계 최대 규모의 수영 축제인 국제수영연맹 세계선수권대회에서 메달을 목에 걸어본 한국인은 단 3명뿐이다.\n",
      "이후 명맥이 끊겼던 한국인 수영세계선수권대회 메달은 2019년 광주 대회의 김수지(울산시청)가 여자 다이빙 1ｍ 스프링보드 동메달로 되살렸다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://v.daum.net/v/20230710070126691\")\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "# 뉴스 제목 가져오기\n",
    "title = soup.find(\"h3\")\n",
    "print(title.string)\n",
    "\n",
    "# 기자 이름 가져오기\n",
    "name = soup.find(\"span\", class_=\"txt_info\")\n",
    "print(name.string)\n",
    "\n",
    "# 작성 날짜 가져오기\n",
    "time = soup.find(\"span\", class_=\"num_date\")\n",
    "print(time.string)\n",
    "\n",
    "# 첫번째 문단\n",
    "para = soup.find(\"p\", attrs={\"dmcf-pid\":\"QcZwLtKw3N\"})\n",
    "print(para.string)\n",
    "\n",
    "para1 = soup.find_all(\"p\", attrs={\"dmcf-ptype\":\"general\"})\n",
    "print(para1[2].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "장마준비\n",
      "여름휴가\n",
      "여성패션\n",
      "남성패션\n",
      "화장품/미용\n",
      "가구/인테리어\n",
      "식품\n",
      "출산/유아동\n",
      "반려동물용품\n",
      "생활/주방용품\n",
      "가전\n"
     ]
    }
   ],
   "source": [
    "# 네이버 쇼핑 카테고리\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://shopping.naver.com/home\")\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "category = soup.find_all(\"li\", class_=\"category_list__y8A1h\")\n",
    "for c in category:\n",
    "    print(c.get_text()) # string으로 못 갖고온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b> The Dormouse's story </b>\n"
     ]
    }
   ],
   "source": [
    "# find(), find_all()\n",
    "# select_one(), select(): css 선택자 사용이 가능하다.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"./story.html\", \"r\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "# print(soup.prettify())\n",
    "\n",
    "# b 태그 찾기\n",
    "b_tag = soup.select_one(\"p.title > b\")\n",
    "print(b_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"story\">\n",
      "      Once upon a time there were three little sisters; and their names were\n",
      "      <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "      ,\n",
      "      <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>\n",
      "      and\n",
      "      <a class=\"sister\" data-io=\"test\" href=\"http://example.com/tillie\" id=\"link2\"> Tillie </a>\n",
      "      ; and they lived at the bottom of a well.\n",
      "    </p>\n"
     ]
    }
   ],
   "source": [
    "p_tag = soup.select_one(\"p.story\")\n",
    "print(p_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"sister\" data-io=\"test\" href=\"http://example.com/tillie\" id=\"link2\"> Tillie </a>\n"
     ]
    }
   ],
   "source": [
    "a_tag = soup.select_one(\"a[data-io='test']\")\n",
    "print(a_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>, <a class=\"sister\" data-io=\"test\" href=\"http://example.com/tillie\" id=\"link2\"> Tillie </a>]\n",
      " Elsie \n",
      " Lacie \n",
      " Tillie \n"
     ]
    }
   ],
   "source": [
    "a_tags = soup.select(\"p.story > a\")\n",
    "print(a_tags)\n",
    "\n",
    "for a in a_tags:\n",
    "    print(a.string)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "황선우, 박태환 이후 12년만의 세계수영선수권 '금빛 역영' 도전\n",
      "이대호\n",
      "2023. 7. 10. 07:01\n",
      "(서울=연합뉴스) 이대호 기자 = 전 세계 최대 규모의 수영 축제인 국제수영연맹 세계선수권대회에서 메달을 목에 걸어본 한국인은 단 3명뿐이다.\n",
      "이후 명맥이 끊겼던 한국인 수영세계선수권대회 메달은 2019년 광주 대회의 김수지(울산시청)가 여자 다이빙 1ｍ 스프링보드 동메달로 되살렸다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://v.daum.net/v/20230710070126691\")\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "# 뉴스 제목 가져오기\n",
    "title = soup.select_one(\"h3\")\n",
    "print(title.string)\n",
    "\n",
    "# 기자 이름 가져오기\n",
    "name = soup.select_one(\"span.txt_info\")\n",
    "print(name.string)\n",
    "\n",
    "# 작성 날짜 가져오기\n",
    "time = soup.select_one(\"span.num_date\")\n",
    "print(time.string)\n",
    "\n",
    "# 첫번째 문단\n",
    "para = soup.select_one(\"p[dmcf-pid='QcZwLtKw3N']\")\n",
    "print(para.string)\n",
    "\n",
    "para1 = soup.select(\"p[dmcf-ptype='general']\")\n",
    "print(para1[2].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally Normal Gifts\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "../img/gifts/img1.jpg\n",
      "../img/gifts/img1.jpg\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "../img/gifts/img2.jpg\n",
      "../img/gifts/img2.jpg\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "../img/gifts/img3.jpg\n",
      "../img/gifts/img3.jpg\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "../img/gifts/img4.jpg\n",
      "../img/gifts/img4.jpg\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n",
      "../img/gifts/img6.jpg\n",
      "../img/gifts/img6.jpg\n",
      "Item Title\n",
      "\n",
      "Description\n",
      "\n",
      "Cost\n",
      "\n",
      "Image\n",
      "\n",
      "\n",
      "Vegetable Basket\n",
      "\n",
      "This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "Now with super-colorful bell peppers!\n",
      "\n",
      "$15.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Russian Nesting Dolls\n",
      "\n",
      "Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! 8 entire dolls per set! Octuple the presents!\n",
      "\n",
      "$10,000.52\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fish Painting\n",
      "\n",
      "If something seems fishy about this painting, it's because it's a fish! Also hand-painted by trained monkeys!\n",
      "\n",
      "$10,005.00\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dead Parrot\n",
      "\n",
      "This is an ex-parrot! Or maybe he's only resting?\n",
      "\n",
      "$0.50\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mystery Box\n",
      "\n",
      "If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. Keep your friends guessing!\n",
      "\n",
      "$1.50\n"
     ]
    }
   ],
   "source": [
    "# https://pythonscraping.com/pages/page3.html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"https://pythonscraping.com/pages/page3.html\")\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "# Totally Normal Gifts 추출\n",
    "title = soup.select_one(\"h1\")\n",
    "print(title.string)\n",
    "\n",
    "# 모든 img 태그 추출(제목 행 옆 이미지 제외)\n",
    "img_tags = soup.select(\"#giftList img\")\n",
    "for img in img_tags:\n",
    "    print(img)\n",
    "    print(img[\"src\"])\n",
    "    print(img.get(\"src\"))\n",
    "\n",
    "# 테이블 내용 추출\n",
    "content = soup.select(\"#giftList\")\n",
    "for item in content:\n",
    "    print(item.get_text().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우울감 (우울증, 스트레스) 대처 순서\n",
      "턱관절 장애. 스트레칭 요고 강추합니다!!\n",
      "[주간 코로나] 05월 3주 - 분석편 -\n",
      "커넥션 (6) 카타고의 참패와 인공지능의 허점\n",
      "[주간 코로나] 05월 3주 - 통계편 - <통권 145호>\n",
      "유튜브 채널 관리(App + Chrome확장팩) 써보니 편하고 좋습니다.\n",
      "윈도우 터미널,CMD 관리자 권한으로 실행하기...\n",
      "로지텍 MX Keys 키보드 백라이트가 자동으로 안꺼질 때\n",
      "소형 잡동사니 정리방법\n",
      "1등급 tv는 어떻게 만들어지나\n",
      "공부 단상 연재 12편\n",
      "시니어를 위한 'EBS 평생학교'를 소개합니다.\n",
      "이미지->PDF 변환 텔레그램 봇 만들기\n",
      "애플워치 손목에 차지 않고 맥북 열기(잠금해제)\n",
      "[주간 코로나] 05월 2주 - 분석편 -\n",
      "정수기 자작 이동식 파우셋\n",
      "SSD 핫딜에 SSD 사려는 SSD 알못이라면 꼭 읽었으면 좋겠읍미다.\n",
      "PDF 파일 크롭하기\n",
      "2023 Ver. 파세코 에어컨 (창문형 에어컨) 설치 후 외부소음차단하기\n",
      "안보시는 TV 수신료 해지 및 환불 방법\n",
      "구글 검색결과 클릭 빗나갈 때 이미지 검색 납치 방지하기\n",
      "서피스 듀오 PTA-VOLTE에서 KT Volte 안되는 현상 해결 건\n",
      "5분이면 충분해! 영어를 몰라도 쉽게 따라할 수 있는 고퀄리티 Ai편집\n",
      "모니터 화면 쉽게 회전. 인텔 그래픽카드 아니어도 피봇,화면 회전 가능\n",
      "인텔 글픽 전용! 모니터 화면회전 : 고개 돌리지 않고 사진보기\n",
      "블렌더 강좌 - 블렌더로 바탕화면 만들기\n",
      "저시력자 가독성 높인 온고딕 서체 (한국장애인개발원 개발)\n",
      "레인지후드 자동화 - 열화상센서 온도 활용, 홈어시스턴트(HA)[내용 추가]\n",
      "[주간 코로나] 05월 2주 - 통계편 - <통권 144호>\n"
     ]
    }
   ],
   "source": [
    "for page_num in range(5):\n",
    "    if page_num == 0:\n",
    "        res = requests.get(\"https://www.clien.net/service/board/lecture\")\n",
    "    else:\n",
    "        res =requests.get(\"https://www.clien.net/service/board/lecture?&od=T31&category=0&po=\"+str(page_num))\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "# title 추출\n",
    "titles= soup.select(\"span.subject_fixed\")\n",
    "for title in titles:\n",
    "    print(title.string.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색결과 list\n",
    "# [{}, {}, {}.....]\n",
    "\n",
    "# 딕셔너리 선언: news_item = {}\n",
    "# 딕셔너리 내용 추가: news_item[\"title\"]=\"파이썬 미래\"\n",
    "\n",
    "# 리스트 선언: news = []\n",
    "# 리스트 추가: append()\n",
    "\n",
    "news_item = {}\n",
    "news = []\n",
    "\n",
    "keyword = input(\"검색어 입력\")\n",
    "\n",
    "url = \"https://news.google.com/search?q=\"+keyword+\"&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, \"lxml\") \n",
    "\n",
    "articles = soup.select(\"div.xrnccd article\")\n",
    "\n",
    "for article in articles:\n",
    "  # 제목\n",
    "  # print(article.select_one(\"h3\").string)\n",
    "    news_item[\"title\"] = article.h3.string\n",
    "  # 링크\n",
    "    news_item[\"link\"] = article.a[\"href\"]\n",
    "  # 작성자\n",
    "    news_item[\"writer\"] = article.img.get(\"alt\")\n",
    "  # 작성날짜\n",
    "    news_item[\"date\"] = article.time.string\n",
    "\n",
    "    news.append(news_item)\n",
    "\n",
    "    # print(title, link, writer, date)\n",
    "\n",
    "print((news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다나와 - 로그인 후 접근할 수 있는 페이지 크롤링\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "form_data = {\n",
    "    \"redirectUrl\": \"https://www.danawa.com/\",\n",
    "    \"loginMemberType\": \"general\",\n",
    "    \"id\": \"pjky5\",\n",
    "    \"isSaveId\": \"true\",\n",
    "    \"password\": \"12344321@a\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"user-agent\": UserAgent().chrome,\n",
    "    \"Referer\": \"https://auth.danawa.com/login?url=https%3A%2F%2Fwww.danawa.com%2F\"\n",
    "}\n",
    "\n",
    "with requests.Session() as s:\n",
    "    # 로그인\n",
    "    res = s.post(\"https://auth.danawa.com/login\", form_data, headers=headers)\n",
    "\n",
    "    # print(res.text)\n",
    "\n",
    "    res = s.get(\"https://buyer.danawa.com/order/Order/orderList\", headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "    # id를 가지고 있는 p 태그 찾기\n",
    "    check_id = soup.find(\"p\", class_=\"user\")\n",
    "    print(check_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "import requests\n",
    "import re\n",
    "\n",
    "wb = Workbook()\n",
    "\n",
    "# 활성화 시트 가져오기\n",
    "ws = wb.active\n",
    "ws.title = \"book\"\n",
    "\n",
    "ws.column_dimensions[\"B\"].width = 30\n",
    "ws.column_dimensions[\"C\"].width = 100\n",
    "ws.column_dimensions[\"D\"].width = 30\n",
    "ws.column_dimensions[\"E\"].width = 30\n",
    "\n",
    "ws.append([\"번호\",\"isbn\",\"도서명\",\"가격\",\"출판사\"])\n",
    "\n",
    "# 네이버 오픈 api + 엑셀 저장\n",
    "headers = {\n",
    "    \"X-Naver-Client-Id\": \"7qWsXKgFmxXhDxIX56ca\",\n",
    "    \"X-Naver-Client-Secret\": \"jsqnAfuYOW\"\n",
    "}\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/book.json\"\n",
    "\n",
    "start, num = 1, 0\n",
    "\n",
    "for idx in range(2):\n",
    "    start_num = start + (idx * 100)\n",
    "\n",
    "params = {\n",
    "    \"query\": \"베르나르 베르베르\",\n",
    "    \"display\": \"100\",\n",
    "    \"start\": str(start_num),\n",
    "}    \n",
    "\n",
    "res = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "data = res.json()\n",
    "\n",
    "for item in data['items']:\n",
    "    num += 1\n",
    "\n",
    "    # 타이틀에서 <b> 태그 제거\n",
    "    title = re.sub(\"(<.*?>)\",\"\",item['title'])\n",
    "\n",
    "    ws.append([num, item['isbn'], item['title'], item['discount'], item['publisher']])\n",
    "    ws.cell(row=num, column=5).hyperlink = item['link']\n",
    "\n",
    "wb.save(\"book.xlsx\")\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.gmarket.co.kr/n/best?viewType=G&groupCode=G06\"\n",
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"20230710컴퓨터전자베스트\"\n",
    "\n",
    "# 특정 열 너비 조절\n",
    "ws.column_dimensions[\"B\"].width = 30 # 회사명\n",
    "ws.column_dimensions[\"C\"].width = 100 # 상품명\n",
    "ws.column_dimensions[\"D\"].width = 30 # 가격\n",
    "ws.column_dimensions[\"E\"].width = 100 # 상세정보url\n",
    "\n",
    "ws.append([\"순위\", \"회사명\", \"상품명\", \"가격\", \"상세정보url\"])\n",
    "\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "best_list = soup.select(\"div.best-list li\")\n",
    "\n",
    "for idx, item in enumerate(best_list, start=1):\n",
    "    product_info = item.select_one(\"a.itemname\")\n",
    "    # 상품명\n",
    "    product_name = product_info.string\n",
    "    # 링크 추출\n",
    "    href = product_info[\"href\"]\n",
    "    # 가격\n",
    "    product_price = item.select_one(\"div.s-price > strong span\").get_text()\n",
    "\n",
    "    product_detail = requests.get(href)\n",
    "    soup2 = BeautifulSoup(product_detail.text, \"lxml\")\n",
    "    \n",
    "    # soup2 안에 담겨있는 회사명 추출\n",
    "    seller = soup2.select_one(\"span.text__brand > .text\")\n",
    "    if not seller:\n",
    "        # 셀러 추출\n",
    "        seller = soup2.select_one(\"span.text__seller\")\n",
    "    \n",
    "    seller_name = seller.get_text().strip()\n",
    "\n",
    "    # print(idx, seller_name, product_name, product_price, href)\n",
    "    ws.append([idx, seller_name, product_name, product_price, href])\n",
    "\n",
    "    ws.cell(row=idx+1, column=5).hyperlink = href\n",
    "\n",
    "wb.save(\"gmarket100.xlsx\")\n",
    "wb.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonsource",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
